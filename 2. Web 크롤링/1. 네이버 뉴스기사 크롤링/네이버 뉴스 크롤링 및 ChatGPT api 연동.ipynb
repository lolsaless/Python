{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gf9P03jUAV_"
      },
      "source": [
        "# Naver 뉴스 기사 크롤링 코드\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFzzSriY71U5"
      },
      "source": [
        "## 필수 라이브러리 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 설치\n",
        "!pip install selenium"
      ],
      "metadata": {
        "id": "-5uZxiO06fdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHd6PJBcvWU4"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium chromium-driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHeb8nE5UGj4"
      },
      "source": [
        "## 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8IsS0mnvtOz"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl1b_lffULII"
      },
      "source": [
        "## page scraping 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtuNkeovvxyj"
      },
      "outputs": [],
      "source": [
        "def safe_find_element(driver, by, value):\n",
        "    try:\n",
        "        return driver.find_element(by, value)\n",
        "    except NoSuchElementException:\n",
        "        return None\n",
        "\n",
        "def news_scraping(news_url, driver):\n",
        "    # 언론사\n",
        "    press_element = safe_find_element(driver, By.XPATH, '//*[@id=\"ct\"]/div[1]/div[1]/a/img[2]')\n",
        "    press = press_element.get_attribute('title') if press_element else \"\"\n",
        "\n",
        "    # 기사 제목\n",
        "    title_element = safe_find_element(driver, By.ID, 'title_area')\n",
        "    title = title_element.text if title_element else \"\"\n",
        "\n",
        "    # 발행일자\n",
        "    date_time_element = safe_find_element(driver, By.XPATH, '//*[@id=\"ct\"]/div[1]/div[3]/div[1]/div/span')\n",
        "    date_time = date_time_element.text if date_time_element else \"\"\n",
        "\n",
        "    # 기자\n",
        "    repoter_element = safe_find_element(driver, By.XPATH, '//*[@id=\"JOURNALIST_CARD_LIST\"]/div[1]/div/div[1]/div/div/div[1]/a[2]/span/span/em')\n",
        "    repoter = repoter_element.text if repoter_element else \"\"\n",
        "\n",
        "    # 기사 본문\n",
        "    article_element = safe_find_element(driver, By.ID, 'dic_area')\n",
        "    article = article_element.text.replace(\"\\n\", \"\").replace(\"\\t\", \"\") if article_element else \"\"\n",
        "\n",
        "    # 기사 반응: 쏠쏠정보\n",
        "    useful_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[1]/a/span[2]')\n",
        "    useful = useful_element.text if useful_element else \"\"\n",
        "\n",
        "    # 기사 반응: 흥미진진\n",
        "    wow_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[2]/a/span[2]')\n",
        "    wow = wow_element.text if wow_element else \"\"\n",
        "\n",
        "    # 기사 반응: 공감백배\n",
        "    touched_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[3]/a/span[2]')\n",
        "    touched = touched_element.text if touched_element else \"\"\n",
        "\n",
        "    # 기사 반응: 분석탁월\n",
        "    analytical_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[4]/a/span[2]')\n",
        "    analytical = analytical_element.text if analytical_element else \"\"\n",
        "\n",
        "    # 기사 반응: 후속강추\n",
        "    recommend_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[5]/a/span[2]')\n",
        "    recommend = recommend_element.text if recommend_element else \"\"\n",
        "\n",
        "    print(\"뉴스:\", [title, press, date_time, repoter, article, useful, wow, touched, analytical, recommend, news_url])\n",
        "\n",
        "    return [title, press, date_time, repoter, article, useful, wow, touched, analytical, recommend, news_url]\n",
        "\n",
        "def scraping(list_url):\n",
        "    driver.implicitly_wait(3)\n",
        "\n",
        "    news_idx = 1\n",
        "    news_df = pd.DataFrame(columns = (\"Title\", \"Press\", \"DateTime\", \"Repoter\", \"Article\", \"Useful\", \"Wow\", \"Touched\", \"Analytical\", \"Recommend\", \"URL\"))\n",
        "\n",
        "    for url in list_url:\n",
        "        driver.get(url)\n",
        "        news_df.loc[news_idx] = news_scraping(url, driver)\n",
        "        news_idx += 1\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "    return news_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GDNnu5EUb80"
      },
      "source": [
        "## 검색 키워드, 크롤링 페이지 설정\n",
        "### 검색창에 검색하고자 하는 키워드 입력 하고, 수집할 페이지는 1에서 2 또는 3페이지 정도로 하시면 최신자료를 수집할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DrPB0lLxEkc"
      },
      "outputs": [],
      "source": [
        "def make_pg_num(num):\n",
        "    \"\"\"Calculate the page number in the format required by the website.\"\"\"\n",
        "    return num if num == 1 else num+9*(num-1)\n",
        "\n",
        "def create_url(search, page_num):\n",
        "    \"\"\"Create a URL with the search term and page number.\"\"\"\n",
        "    return f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=17&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={page_num}\"\n",
        "\n",
        "def make_urls(search, start_pg, end_pg):\n",
        "    \"\"\"Generate the URLs for the range of pages.\"\"\"\n",
        "    return [create_url(search, make_pg_num(i)) for i in range(start_pg, end_pg+1)]\n",
        "\n",
        "def input_with_validation(prompt):\n",
        "    \"\"\"Ask for input with the given prompt, repeating until a valid integer is provided.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            return int(input(prompt))\n",
        "        except ValueError:\n",
        "            print(\"Invalid input, please enter an integer.\")\n",
        "\n",
        "def main():\n",
        "    search = input(\"검색 키워드를 입력해주세요: \")\n",
        "\n",
        "    start_pg = input_with_validation(\"\\n크롤링 시작 페이지를 입력해주세요. ex)1(숫자만 입력): \")\n",
        "    print(f\"\\n크롤링 시작 페이지: {start_pg}페이지\")\n",
        "\n",
        "    end_pg = input_with_validation(\"\\n크롤링 종료 페이지를 입력해주세요. ex)1(숫자만 입력): \")\n",
        "    print(f\"\\n크롤링 종료 페이지: {end_pg}페이지\")\n",
        "\n",
        "    return make_urls(search, start_pg, end_pg)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    search_urls = main()\n",
        "    print(\"생성된 URL: \", search_urls)\n",
        "\n",
        "#Chrome drive option 설정\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "\n",
        "chrome_options.add_argument('--verbose')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--disable-gpu')\n",
        "chrome_options.add_argument('--windows-size=1920, 1200')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "driver = webdriver.Chrome(options = chrome_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAhQTLQpUuHO"
      },
      "source": [
        "## 기사 링크 수집"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR6yj9mrxKFR"
      },
      "outputs": [],
      "source": [
        "# Initialize the list to store the links\n",
        "list_url = []\n",
        "\n",
        "# Iterate over the URLs\n",
        "for url in search_urls:\n",
        "    # Send GET request to the web page\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # If the request is successful, extract the HTML content and create a BeautifulSoup object\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        links = soup.select(\"a.info, a.sub_txt\")  # Select both \"a.info\" and \"a.sub_txt\" elements\n",
        "\n",
        "        # Filter and save the links with \"naver.com\" in their address\n",
        "        for link in links:\n",
        "            href = link.get(\"href\")\n",
        "            if \"naver.com\" in href:\n",
        "                list_url.append(href)\n",
        "    else:\n",
        "        print(\"The request failed.\")\n",
        "\n",
        "    # Sleep for 1 second\n",
        "    time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPndM46D8nXa"
      },
      "outputs": [],
      "source": [
        "list_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqruP_4UU0Un"
      },
      "source": [
        "## 크롤링 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rweCr9lzFZb"
      },
      "outputs": [],
      "source": [
        "news_df = scraping(list_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl7sm_Xn5axP"
      },
      "outputs": [],
      "source": [
        "news_df.to_excel(\"news.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ8-VOxcoFi9"
      },
      "source": [
        "# OpenAI API 연동\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8KOf97maN1i"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word를 다룰 수 있는 라이브러리 설치"
      ],
      "metadata": {
        "id": "CPi_9Yro9Wgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "opDQ942Y9T_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ostcv_0W5V"
      },
      "source": [
        "## GPT 기사 분석: 대화형\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubx22TgxCgNS"
      },
      "source": [
        "대화를 통해 자유롭게 묻고 답할 수 있다.<br>\n",
        "GPT에게 유능한 저널리스트이자 텍스트 분석 전문가라는 역할을 부여함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YvXEOuRj8s2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "# Assuming that you have the DataFrame 'news_df' already loaded\n",
        "def ask_from_article(index):\n",
        "    article = news_df['Article'][index]\n",
        "\n",
        "    openai.api_key = \"여기에 openai에서 발급받은 api key를 붙여넣기 하세요.\"\n",
        "\n",
        "    # 역할 부여(유능한 기자이자, 텍스트 분석 전문가)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a very competent journalist and text analytics expert who needs to do the following.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Here is an article: {article}\"}\n",
        "    ]\n",
        "\n",
        "    while True:\n",
        "        user_content = input(\"기사에 대한 질문을 입력하세요. : \")\n",
        "        if user_content.lower() == \"종료\":\n",
        "            break\n",
        "        messages.append({\"role\" : \"user\", \"content\" : f\"{user_content}\"})\n",
        "\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model = \"gpt-3.5-turbo\",\n",
        "            messages = messages\n",
        "        )\n",
        "\n",
        "        assistant_content = completion.choices[0].message[\"content\"].strip()\n",
        "\n",
        "        messages.append({\"role\" : \"assistant\", \"content\" : f\"{assistant_content}\"})\n",
        "\n",
        "        print(f\"GPT-3.5 Turbo : {assistant_content}\")\n",
        "\n",
        "    # Saving the conversation to a word file\n",
        "    doc = Document()\n",
        "    for message in messages:\n",
        "        doc.add_paragraph(f\"{message['role']} : {message['content']}\")\n",
        "    doc.save(\"대화기록.docx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO3V0Kx9mVls"
      },
      "outputs": [],
      "source": [
        "# 대화를 종료할 때는 \"종료\" 입력\n",
        "# ask_from_article() 함수에 요약하고 싶은 기사의 번호를 괄호에 입력하고, Ctrl + Enter\n",
        "\n",
        "ask_from_article(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCycSLwhwRtp"
      },
      "source": [
        "## GPT 기사 분석: 자동 반복"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEfKzUmr0DSt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "import time\n",
        "from docx import Document\n",
        "\n",
        "def get_article_content(index):\n",
        "    # Replace this with your own logic to retrieve the article content from `news_df`\n",
        "    return news_df['Article'].iloc[index]\n",
        "\n",
        "def summarize_article(index, doc):\n",
        "    try:\n",
        "        article = get_article_content(index)\n",
        "\n",
        "        openai.api_key = \"여기에 openai에서 발급받은 api key를 붙여넣기 하세요.\"\n",
        "\n",
        "        # 역할 부여(유능한 기자이자, 텍스트 분석 전문가)\n",
        "        messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a very competent journalist and text analytics expert who needs to do the following.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"You should briefly summarize the article and write your answer in Korean. You should also create a positive/negative: and indicate positive if the article is positive, and negative if it is negative.: {article}\"}\n",
        "        ]\n",
        "\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model = \"gpt-3.5-turbo\",\n",
        "            messages = messages\n",
        "        )\n",
        "\n",
        "        assistant_content = completion.choices[0].message[\"content\"].strip()\n",
        "\n",
        "        print(f\"Summary of article {index}: {assistant_content}\")\n",
        "\n",
        "        # Adding the summary to the Word document\n",
        "        doc.add_paragraph(f\"Summary of article {index}: {assistant_content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {index}: {str(e)}\")\n",
        "\n",
        "# Create the Word document\n",
        "doc = Document()\n",
        "\n",
        "# Call function for each article with a 20 second delay\n",
        "for i in range(len(news_df['Article'])):\n",
        "    summarize_article(i, doc)\n",
        "    time.sleep(20)\n",
        "\n",
        "# Save the Word document\n",
        "doc.save(\"summaries.docx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}