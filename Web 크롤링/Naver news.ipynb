{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "Chrome_options = Options()\n",
    "Chrome_options.add_argument(\"--verbose\")\n",
    "Chrome_options.add_argument('--no-sandbox')\n",
    "Chrome_options.add_argument(\"--headless=new\")\n",
    "Chrome_options.add_argument(\"--disable-gpu\")\n",
    "Chrome_options.add_argument(\"--windows-size=1920,1200\")\n",
    "Chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "Chrome_options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "driver = webdriver.Chrome(options=Chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_find_element(driver, by, value):\n",
    "    try:\n",
    "        return driver.find_element(by, value)\n",
    "    except NoSuchElementException:\n",
    "        return None\n",
    "\n",
    "def news_scraping(news_url, driver):\n",
    "    # 언론사\n",
    "    press_element = safe_find_element(driver, By.XPATH, '//*[@id=\"ct\"]/div[1]/div[1]/a/img[2]')\n",
    "    press = press_element.get_attribute('title') if press_element else \"\"\n",
    "\n",
    "    # 기사 제목\n",
    "    title_element = safe_find_element(driver, By.ID, 'title_area')\n",
    "    title = title_element.text if title_element else \"\"\n",
    "\n",
    "    # 발행일자\n",
    "    date_time_element = safe_find_element(driver, By.XPATH, '//*[@id=\"ct\"]/div[1]/div[3]/div[1]/div/span')\n",
    "    date_time = date_time_element.text if date_time_element else \"\"\n",
    "\n",
    "    # 기자\n",
    "    repoter_element = safe_find_element(driver, By.XPATH, '//*[@id=\"JOURNALIST_CARD_LIST\"]/div[1]/div/div[1]/div/div/div[1]/a[2]/span/span/em')\n",
    "    repoter = repoter_element.text if repoter_element else \"\"\n",
    "\n",
    "    # 기사 본문\n",
    "    article_element = safe_find_element(driver, By.ID, 'dic_area')\n",
    "    article = article_element.text.replace(\"\\n\", \"\").replace(\"\\t\", \"\") if article_element else \"\"\n",
    "\n",
    "    # 기사 반응: 쏠쏠정보\n",
    "    useful_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[1]/a/span[2]')\n",
    "    useful = useful_element.text if useful_element else \"\"\n",
    "\n",
    "    # 기사 반응: 흥미진진\n",
    "    wow_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[2]/a/span[2]')\n",
    "    wow = wow_element.text if wow_element else \"\"\n",
    "\n",
    "    # 기사 반응: 공감백배\n",
    "    touched_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[3]/a/span[2]')\n",
    "    touched = touched_element.text if touched_element else \"\"\n",
    "\n",
    "    # 기사 반응: 분석탁월\n",
    "    analytical_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[4]/a/span[2]')\n",
    "    analytical = analytical_element.text if analytical_element else \"\"\n",
    "\n",
    "    # 기사 반응: 후속강추\n",
    "    recommend_element = safe_find_element(driver, By.XPATH, '//*[@id=\"likeItCountViewDiv\"]/ul/li[5]/a/span[2]')\n",
    "    recommend = recommend_element.text if recommend_element else \"\"\n",
    "\n",
    "    print(\"뉴스:\", [title, press, date_time, repoter, article, useful, wow, touched, analytical, recommend, news_url])\n",
    "\n",
    "    return [title, press, date_time, repoter, article, useful, wow, touched, analytical, recommend, news_url]\n",
    "\n",
    "def scraping(list_url):\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    news_idx = 1\n",
    "    news_df = pd.DataFrame(columns = (\"Title\", \"Press\", \"DateTime\", \"Repoter\", \"Article\", \"Useful\", \"Wow\", \"Touched\", \"Analytical\", \"Recommend\", \"URL\"))\n",
    "\n",
    "    for url in list_url:\n",
    "        driver.get(url)\n",
    "        news_df.loc[news_idx] = news_scraping(url, driver)\n",
    "        news_idx += 1\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pg_num(num):\n",
    "    \"\"\"Calculate the page number in the format required by the website.\"\"\"\n",
    "    return num if num == 1 else num+9*(num-1)\n",
    "\n",
    "def create_url(search, page_num):\n",
    "    \"\"\"Create a URL with the search term and page number.\"\"\"\n",
    "    return f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=17&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={page_num}\"\n",
    "\n",
    "def make_urls(search, start_pg, end_pg):\n",
    "    \"\"\"Generate the URLs for the range of pages.\"\"\"\n",
    "    return [create_url(search, make_pg_num(i)) for i in range(start_pg, end_pg+1)]\n",
    "\n",
    "def input_with_validation(prompt):\n",
    "    \"\"\"Ask for input with the given prompt, repeating until a valid integer is provided.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            return int(input(prompt))\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, please enter an integer.\")\n",
    "\n",
    "def main():\n",
    "    search = input(\"검색 키워드를 입력해주세요: \")\n",
    "\n",
    "    start_pg = input_with_validation(\"\\n크롤링 시작 페이지를 입력해주세요. ex)1(숫자만 입력): \")\n",
    "    print(f\"\\n크롤링 시작 페이지: {start_pg}페이지\")\n",
    "\n",
    "    end_pg = input_with_validation(\"\\n크롤링 종료 페이지를 입력해주세요. ex)1(숫자만 입력): \")\n",
    "    print(f\"\\n크롤링 종료 페이지: {end_pg}페이지\")\n",
    "\n",
    "    return make_urls(search, start_pg, end_pg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_urls = main()\n",
    "    print(\"생성된 URL: \", search_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to store the links\n",
    "list_url = []\n",
    "\n",
    "# Iterate over the URLs\n",
    "for url in search_urls:\n",
    "    # Send GET request to the web page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the request is successful, extract the HTML content and create a BeautifulSoup object\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.select(\"a.info, a.sub_txt\")\n",
    "\n",
    "        # Filter and save the links with \"naver.com\" in their address\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if \"naver.com\" in href:\n",
    "                list_url.append(href)\n",
    "    else:\n",
    "        print(\"The request failed.\")\n",
    "\n",
    "    # Sleep for 1 second\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping(list_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
